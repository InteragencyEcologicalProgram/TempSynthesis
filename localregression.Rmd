---
title: "Local Regression analysis"
author: "Rosie"
date: "7/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lubridate)
library(tidyverse)
library(vegan)
library(sf)
if (!require("rspatial")) devtools::install_github('rspatial/rspatial')
library(rspatial)
library( spgwr )
library(nlme)
#load the data
temps = readRDS("Temp_filtered (1).rds")


#first calculate the daily means
tempmean = temps %>%
  filter(Date > as.Date("2014-1-1"), Station != "RYF") %>%
  mutate(julian = yday(Date)) %>%
  group_by(Station, julian) %>%
  summarize(Temp = mean(Temp, na.rm = T))

```

## Local regression

One of the cool ways you can address differences across space is run a regression that calculates different coefficients over space. I ran the model over the continuous stations, then extrapolated the results to the rest of the Detla.

First we have to do some data manipulation to get everything into a spatial format. 

```{r}
#read in shapefile of the delta
delta = read_sf("DeltaShapefile/hydro_delta_marsh.shp")

#add lat/longs for the stations
stas = read.csv("StationLatLongs.csv")


#attached lat/longs to mean temperature
tempmean2 = merge(tempmean, stas)

#Specify a coordinate reference system and turn it into a spatial object
alb <- CRS("+proj=aea +lat_1=34 +lat_2=40.5 +lat_0=0 +lon_0=-120 +x_0=0 +y_0=-4000000 +ellps=GRS80 +datum=NAD83 +units=m +no_defs")

#Filter the Delta shapefile so it doesn't include areas where we dont have a lot of data
delta2 = sf::as_Spatial(dplyr::filter(delta, HNAME != "SAN FRANCISCO BAY", HNAME != "SAN PABLO BAY"))

#I'm not positive we have to do this transformation, but it was in teh example so I'm doing it.
ctst <- spTransform(delta2, alb)
```

Now I'll filter out just the data from January-June and find the optimal badwidth for the interpolation. I think this basically tells you how much "smoothing" to put between your sampling points.
```{r}
tempmean2.1 = dplyr::filter(tempmean2, julian <190)
tempmeansub = sample_n(tempmean2.1, 1000)
sp = tempmeansub
coordinates(sp) = ~ Longitude + Latitude
crs(sp) <- "+proj=longlat +datum=NAD83"
spt <- spTransform(sp, alb)

#find optimal bandwith
#THIS TAKES FOREVER!
bw <- gwr.sel( Temp~ julian, data= spt)


```

To make the pretty map, I need to create a blank raster from the Delta shapefile. Once I calculate the predicted values for the regression, I'll fill it in.


```{r}
r <- raster(ctst,  res=100)
rdelta2 <- rasterize(ctst, r)
newpts <- rasterToPoints(rdelta2)

#Turn Newpoints into a spatial grid
newpts2 = as.data.frame(newpts)
coordinates(newpts2) = ~ x  + y
crs(newpts2) = alb

```

Now I run the GWR function to calculate local regressions

```{r warning=F}
#

g <- gwr(Temp ~ julian, data= spt,
         bandwidth=bw, fit.points=newpts[, 1:2])
g

resultsg = as.data.frame(g$SDF)

#sigTest = abs(g$SDF$julian) -2 * g$SDF$julian_se 

#Note: i'd like to run it with "se.fit = T" so I can calculate the 
#standard error, but it doens't seem to be working. I had it running 
#all weekend, and noting. 

```

Put the fitted points back on my raster and plot it!

```{r fig.width=15}
slope <- rdelta2
intercept <- rdelta2
slope[!is.na(slope)] <- g$SDF$julian
intercept[!is.na(intercept)] <- g$SDF$'(Intercept)'
s <- stack(intercept, slope)
names(s) <- c('intercept', 'slope')
plot(s, xlim = c(-200000, -100000), ylim = c(-50000,50000))


```

##Now the cool part - how things have changed over the historical record

```{r}


#first calculate the daily means
temps$Year = year(temps$Date)
tempmax = temps %>%
  mutate(julian = yday(Date)) %>%
  group_by(Station, julian, Year, Date) %>%
  summarize(Temp = mean(Temp, na.rm = T),
            maxTemp = max(Temp, na.rm = T),
            minTemp = min(Temp, na.rm = T))

tempmax2 = merge(tempmax, stas)

tempmax2.1 = dplyr::filter(tempmax2, julian <250)
#spx = sample_n(tempmax2.1, 10000)
spx = tempmax2.1

#turn it into a spatial object
coordinates(spx) = ~ Longitude + Latitude
crs(spx) <- "+proj=longlat +datum=NAD83"
sptx <- spTransform(spx, alb)

#find badwidth
bwx <- gwr.sel( maxTemp~ julian + Year, data= sptx)


gx <- gwr(maxTemp~ julian + Year, data= sptx,
         bandwidth=bwx, fit.points=newpts[, 1:2])
gx


```

Put the fitted points back on my raster and plot it!

```{r fig.width=15}
slopeDay <- rdelta2
slopeYear <- rdelta2
intercept <- rdelta2
slopeDay[!is.na(slopeDay)] <- gx$SDF$julian
slopeYear[!is.na(slopeYear)] <- gx$SDF$Year
intercept[!is.na(intercept)] <- gx$SDF$'(Intercept)'
s <- stack(intercept, slopeDay, slopeYear)
names(s) <- c('intercept', 'Seasonal Change', "Inter-annual Change")
plot(s, xlim = c(-200000, -100000), ylim = c(-50000,50000))


```

## Temporal autocorrelation

I'm not sure I need to worry about temporal autocorrelation if time is part of my model, but I'll do some testing.

```{r}

testdat = filter(tempmax2.1, Station == first(Station)) %>%
arrange(Date)

acf(testdat$maxTemp)

pacf(testdat$maxTemp)

lmtest = lm(maxTemp~ julian + Year, data = testdat)

summary(lmtest)

acf(lmtest$residuals)

pacf(lmtest$residuals)

#find the start value of rho
acf(resid(lmtest), plot=FALSE)$acf[2]

# I'm confused and don't know what i'm diong
library(itsadug)
library(mgcv)

lmtest2 = gls(maxTemp~ julian + Year, data = testdat, correlation = corAR1(form = ~ Date))

acf(lmtest2$residuals)
acf_resid(lmtest2)


#try a gam
m1 = bam(maxTemp ~ s(julian) + s(Year), data=testdat)
acf(m1$residuals)

summary(m1)

#calculate start value of Rho
r1 = acf(resid(m1), plot=FALSE)$acf[2]

#identify the start of the time series
testdat = start_event(testdat, column="Date", event=c("Station", "Year"), label.event="Event")

m1AR1 = bam(maxTemp ~ s(julian) + s(Year), data=testdat, rho=r1, AR.start=testdat$start.event)

summary(m1AR1)
acf_resid(m1AR1)


```
So, I may want to try and use a GAM or something. Not sure.


## Package GWmodel

Before I investigate spatial GAMs, I want to try a different package for the geographically weighted regression.

First, compute the distances between the points on the grid where the model will be estimated, and the station locations. This takes a while, but if it is pre-computed, successive applications of GWR functions are speeded up.
```{r}
library(GWmodel)

DM <- gw.dist(dp.locat=coordinates(spt),rp.locat=coordinates(newpts2))


#now the model
gwr.res <- gwr.basic(Temp~ julian, data= spt, regression.points=newpts2, bw=bw, dMat=DM,kernel='gaussian')

gwr.res

```

Put the fitted points back on my raster and plot it!

```{r fig.width=15}
slopeDay <- rdelta2
intercept <- rdelta2
slopeDay[!is.na(slopeDay)] <- gwr.res$SDF$julian
#slopeYear[!is.na(slopeYear)] <- gx$SDF$Year
intercept[!is.na(intercept)] <- gwr.res$SDF$Intercept
s <- stack(intercept, slopeDay)
names(s) <- c('intercept', 'Seasonal Change')
plot(s, xlim = c(-200000, -100000), ylim = c(-50000,50000))

```

Estimate standard errors using bootstrapping

```{r}

bsm.res1 <- gwr.bootstrap(Temp~julian, kernel = "gaussian", sptx, R=99)
bsm.res1

library(boot)
set.seed(4676)

gwrcoef <- function(hpdf,i) gwr.basic, data=spt[i,], regression.points=newpts2, bw=bw, dMat=DM[i,],kernel='gaussian')$SDF$PROF

bootres <- boot(spt,gwrcoef,100)
gwr.res$SDF$bsePROF <- sqrt(apply(bootres$t,2,var))
image(gwr.res$SDF,'bsePROF')
plot(londonborough,add=TRUE)

###UGH!! NONE OF THIS IS WORKING!!!

```
Maybe I should just use seasonal means so I don' tneed to worry about the temporal autocorrelation

```{r}

#filter out just the summer months and calculate mean, min max for the season
summer = dplyr::filter(tempmax2, julian <244, julian >153) %>%
  group_by(Year, Station, Latitude, Longitude) %>%
  summarize(maxTemp = mean(maxTemp, na.rm = T), minTemp = mean(minTemp, na.rm = T), Temp = mean(Temp, na.rm = T))
#spx = sample_n(tempmax2.1, 10000)
sppt = summer

#turn it into a spatial object
coordinates(sppt) = ~ Longitude + Latitude
crs(sppt) <- "+proj=longlat +datum=NAD83"
sppt <- spTransform(sppt, alb)

#find badwidth
bwx <- gwr.sel( maxTemp~ Year, data= sppt)


DM <- gw.dist(dp.locat=coordinates(sppt),rp.locat=coordinates(newpts2))


#now the model
gwr.res <- gwr.basic(Temp~ Year, data= sppt, regression.points=newpts2, bw=bwx, dMat=DM,kernel='gaussian')

gwr.res

#see what the other package gives us
gx <- gwr(maxTemp~ Year, data= sppt,
         bandwidth=bwx, fit.points=newpts[, 1:2])
gx

slopeYear <- rdelta2
intercept <- rdelta2
slopeYear[!is.na(slopeYear)] <- gwr.res$SDF$Year
intercept[!is.na(intercept)] <- gwr.res$SDF$Intercept
s <- stack(intercept, slopeYear)
names(s) <- c('intercept', 'long-term change')
plot(s, xlim = c(-200000, -100000), ylim = c(-50000,50000))



bsm.res1 <- gwr.bootstrap(Temp~Year, kernel = "gaussian", sppt, R=99)
bsm.res1
#UGH still not working


```
## GAMs

```{r}
tempmax2.1 = arrange(tempmax2.1, Station, Date) %>%
  filter(Year >1995)

g1 = bam(Temp ~ s(julian) + s(Year) + s(Latitude, Longitude), 
data = tempmax2.1, method = "REML")

summary(g1)

vis.gam(g1, view = c("Longitude", "Latitude"), plot.type = "contour",
        too.far = 0.1)

vis.gam(g1, view = c("Longitude", "Latitude"), plot.type = "persp",
        too.far = 0.1, se = 2)

#use a tensor product smooth for an interaction between 
#space and time

g2 = bam(Temp ~ s(julian) + s(Year) + 
           te(Latitude, Longitude, Year), 
data = tempmax2.1, method = "REML")

summary(g2)

#now the interaction version
#curvalinierity
g3 = bam(Temp ~ s(julian) + 
           te(Latitude, Longitude, julian),
data = tempmax2.1, method = "REML")

summary(g3)
plot(g3)

#try adding the temporal correlation term

#calculate start value of Rho
r1 = acf(resid(g3), plot=FALSE)$acf[2]

#identify the start of the time series
testdat = start_event(tempmax2.1, column="Date", event=c("Station", "Year"), label.event="Event")

g4 = bam(Temp ~ s(julian) + s(Year) + 
           ti(Latitude, Longitude, Year)+ 
           ti(Latitude, Longitude, julian), 
data =testdat, method = "REML",  rho=r1, AR.start=testdat$start.event)

summary(g4)
plot(g4)
acf(resid(g4))
acf_resid(g4, split_pred = c("Year"))

#now do it with maximum temperatures

g3x = bam(maxTemp ~  
           te(Latitude, Longitude, Year) +
           te(Latitude, Longitude, julian), 
data = tempmax2.1, method = "REML")
#calculate start value of Rho
r1x = acf(resid(g3x), plot=FALSE)$acf[2]


g4x = bam(maxTemp ~ s(julian) + s(Year) + 
           ti(Latitude, Longitude, Year)+ 
           ti(Latitude, Longitude, julian), 
data =testdat, method = "REML",  rho=r1x, AR.start=testdat$start.event)

summary(g4x)
plot(g4x)
acf_resid(g4x)

vis.gam(g4x, view = c("Longitude", "Latitude"), plot.type = "contour",
        too.far = 0.1)


g4x2 = bam(maxTemp ~ 
           te(Latitude, Longitude, Year)+ 
           te(Latitude, Longitude, julian), 
data =testdat, method = "REML",  rho=r1x, AR.start=testdat$start.event)

summary(g4x2)
plot(g4x2)
acf_resid(g4x2)

vis.gam(g4x, view = c("Longitude", "Latitude"), plot.type = "contour",
        too.far = 0.1)


#gamtabs(g4x, type = 'html')

#If I don't have a lot of stations with the long time series, ditch spatial structure and move to random effects. 

```


